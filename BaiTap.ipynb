{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1JYFxCYJsVLw0s2L5Ft2zAZdZrpgiEQLC","authorship_tag":"ABX9TyMktdaS9bXqUyrYASr6R5ux"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HZCbPdaC2mds","executionInfo":{"status":"ok","timestamp":1760179374804,"user_tz":-420,"elapsed":18994,"user":{"displayName":"PhÃº QuÃ½ Äá»—","userId":"17478058417784139154"}},"outputId":"8fb3b0df-689d-4831-aca5-393731f9de19"},"outputs":[{"output_type":"stream","name":"stdout","text":["Nháº­p cÃ¢u há»i: Nhá»¯ng ngÃ¢n hÃ ng nÃ o Ä‘Ã£ tÄƒng lÃ£i suáº¥t Ä‘áº§u nÄƒm 2025?\n","\n","ğŸ” Káº¿t quáº£ truy xuáº¥t:\n","\n","--- Káº¿t quáº£ 1 (Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng = 0.5296) ---\n","Nhá»¯ng ngÃ¢n hÃ ng nÃ o Ä‘Ã£ tÄƒng lÃ£i suáº¥t huy Ä‘á»™ng Ä‘áº§u nÄƒm 2025? Ãt nháº¥t 7 ngÃ¢n hÃ ng Ä‘Ã£ tÄƒng lÃ£i suáº¥t huy Ä‘á»™ng gá»“m Agribank, Bac A Bank, NCB, Eximbank, KienlongBank... chá»§ yáº¿u á»Ÿ ká»³ háº¡n dÃ i tá»« 12 thÃ¡ng trá»Ÿ lÃªn. ...\n","\n","--- Káº¿t quáº£ 2 (Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng = 0.4759) ---\n"," CÃ³ bao nhiÃªu ngÃ¢n hÃ ng Ä‘Ã£ tÄƒng lÃ£i suáº¥t huy Ä‘á»™ng tá»« Ä‘áº§u thÃ¡ng 11? 13 ngÃ¢n hÃ ng ...\n","\n","--- Káº¿t quáº£ 3 (Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng = 0.3296) ---\n","Ngay tá»« Ä‘áº§u nÄƒm 2025, nhiá»u NHTM Ä‘Ã£ tÄƒng lÃ£i suáº¥t huy Ä‘á»™ng tiá»n gá»­i, triá»ƒn khai cÃ¡c chÆ°Æ¡ng trÃ¬nh Æ°u Ä‘Ã£i Ä‘á»ƒ thu hÃºt khÃ¡ch hÃ ng gá»­i tiáº¿t kiá»‡m. Tá»« Ä‘áº§u thÃ¡ng 1/2025 Ä‘áº¿n nay, Ä‘Ã£ cÃ³ Ã­t nháº¥t 7 ngÃ¢n hÃ ng tÄƒng lÃ£i suáº¥t huy Ä‘á»™ng trong Ä‘Ã³ cÃ³ Agribank, Bac A Bank, NCB, Eximbank, KienlongBank... CÃ¡c ngÃ¢n hÃ ng nÃ y chá»§ yáº¿u Ä‘iá»u chá»‰nh tÄƒng lÃ£i suáº¥t á»Ÿ cÃ¡c ká»³ háº¡n dÃ i tá»« 12 thÃ¡ng trá»Ÿ lÃªn. Eximbank hiá»‡n Ä‘ang Ã¡p dá»¥ng má»©c lÃ£i suáº¥t tá»« 6,5% - 6,8%/nÄƒm cho cÃ¡c khoáº£n tiá»n gá»­i cÃ³ ká»³ háº¡n tá»« 15 Ä‘áº¿n 34 thÃ¡ng. ÄÃ¢y lÃ  má»©c lÃ£i  ...\n"]}],"source":["# pipeline_retrieval_bankqa.py\n","import json\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","\n","# 1ï¸âƒ£ Äá»c file dataset\n","with open(\"/content/drive/MyDrive/bank_qa_laisuat_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n","    data = json.load(f)\n","\n","# 2ï¸âƒ£ TrÃ­ch xuáº¥t toÃ n bá»™ vÄƒn báº£n tá»« cÃ¡c pháº§n khÃ¡c nhau\n","def extract_text(data):\n","    documents = []\n","\n","    for item in data:\n","        # --- BÃ i bÃ¡o ---\n","        if isinstance(item, dict) and \"content\" in item:\n","            documents.append(item[\"content\"])\n","\n","            # ThÃªm QA pairs náº¿u cÃ³\n","            if \"qa_pairs\" in item:\n","                for qa in item[\"qa_pairs\"]:\n","                    documents.append(f\"{qa['question']} {qa['answer']}\")\n","\n","        # --- CÃ´ng Ä‘iá»‡n ---\n","        elif isinstance(item, dict) and \"document\" in item:\n","            doc = item[\"document\"]\n","            ctx = item.get(\"context\", {})\n","            directives = item.get(\"directives\", {})\n","            merged_text = f\"{doc['title']} {doc.get('purpose', '')} {ctx.get('background', '')}\"\n","            documents.append(merged_text)\n","\n","        # --- BÃ¡o cÃ¡o NHNN ---\n","        elif isinstance(item, dict) and \"summary\" in item:\n","            documents.append(item[\"summary\"])\n","\n","        # --- Máº£ng QA Ä‘á»™c láº­p ---\n","        elif isinstance(item, list):\n","            for qa in item:\n","                if \"question\" in qa and \"answer\" in qa:\n","                    context = qa.get(\"context\", \"\") # Get context, default to empty string if not present\n","                    documents.append(f\"{context} {qa['question']} {qa['answer']}\")\n","\n","    return documents\n","\n","documents = extract_text(data)\n","\n","# 3ï¸âƒ£ XÃ¢y TF-IDF vectorizer\n","vectorizer = TfidfVectorizer(stop_words=None)\n","tfidf_matrix = vectorizer.fit_transform(documents)\n","\n","# 4ï¸âƒ£ HÃ m truy váº¥n\n","def retrieve(query, top_k=3):\n","    query_vec = vectorizer.transform([query])\n","    sims = cosine_similarity(query_vec, tfidf_matrix).flatten()\n","    top_indices = sims.argsort()[-top_k:][::-1]\n","    results = [(documents[i], float(sims[i])) for i in top_indices]\n","    return results\n","\n","# 5ï¸âƒ£ Thá»­ nghiá»‡m\n","if __name__ == \"__main__\":\n","    query = input(\"Nháº­p cÃ¢u há»i: \")\n","    results = retrieve(query)\n","    print(\"\\nğŸ” Káº¿t quáº£ truy xuáº¥t:\")\n","    for i, (text, score) in enumerate(results, 1):\n","        print(f\"\\n--- Káº¿t quáº£ {i} (Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng = {score:.4f}) ---\")\n","        print(text[:500], \"...\")"]}]}